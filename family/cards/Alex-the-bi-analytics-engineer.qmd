---
title: "Alex the BI/Analytics Engineer"
toc-expand: 2
---

::: grid
::: {.g-col-12 .g-col-xl-3}
<img src="/img/tammy.jpg" width="200" height="200"/>
:::

::: {.g-col-12 .g-col-xl-8}
-   Alex **needs** to apply software engineering best practices to analytics code to ensure clean, transformed data are ready for analysis.
-   They **struggle** with getting access to the data they need and with creating structure out of unstructured data pulled from different sources
-   **We can help** them by centralizing the storage of data; providing tools for data modeling, testing, and cataloging, with training on how to use those tools; enabling direct access to this data for analysts, data scientists and other users; and enabling people to share their work products to reduce overhead for future work
:::
:::

::: lightblue-highlight
## Alex needs tools to manage data discovery, lineage, and transformation

Alex’s job is to empower people to answer questions by building data tables and providing clean, transformed datasets. An embedded business analyst needs some data about medication use at Fred Hutch? Boom, Alex is there with an analysis-ready dataset and accompanying documentation. A clinical analyst needs new tables built for a Tableau dashboard? A researcher needs data from their EHR transformed into the format of a specific biospecimen database? Bam, Alex is on it like green on grass—anything to move work at Fred Hutch forward. But Alex’s data is sourced from a lot of different places, and they have no good tools for discovering all available data, managing reference data, or tracking data sources and lineage. Additionally, the data sets are all structured differently, and sometimes there are discrepancies across reports in how objects are defined, resulting in conflicting reports or broken code. Alex will never forget the time their clinical analyst colleagues had to figure out why two reports on patients with brain metastases, created by different departments but ostensibly using the same kinds of data, didn't match. It took weeks of long emails and redundant work to find out that the problem was because some key Epic workflows had changed, but only a few people knew about it. Alex built some new objects after the fact to make sure it never happened again, but they still worry that some other case like this could spring up. It would speed Alex’s work up considerably if they could get data from a single centralized platform, and be able to ingest data more easily from outside sources, and have better tools for making sure everyone at Fred Hutch uses consistent data definitions. Sometimes Alex runs into a thorny dilemma or an area where they’re not sure of the best practices or right tools, and they wish they had peers to talk it through with. But Alex doesn’t know anyone else in Fred Hutch who does the same kind of work – it would be great if they had a way to find out.
:::

::: darkblue-highlight
Collaborators: Clinical analyst, Financial analyst, [Data Scientist](/family/cards/Tammy-the-data-scientist.html), [Carina the Clinical Researcher](/family/cards/Carina-the-clinical-researcher.html)

Downstream users: Service Line Managers and clinical leaders, Embedded business analysts
:::

# Key Challenges

-   Understanding the landscape of clinical data applications at Fred Hutch, where data is stored, and how to acquire access

-   Local machines are not the best computing environment for clinical data science; some clinical databases cannot be accessed from a Mac and many computing environments for reproducible analysis cannot be re-created on Windows

-   Educating and nudging researchers towards best practices for clinical data science

-   Lack of self-service tools for researchers mean that the Tammy spends more time building one-off datasets rather than data science tools that can be used by many researchers

-   There is no way to clearly attach information about data use agreements and access permissions to each dataset or project

-   Availability of time and staff limits pace and volume of help provided

-   There is no unified system with all the relevant data; data must be collated from multiple systems

# Needs and Wants

-   An efficient way to store and retrieve past models/queries for future reference

-   A more efficient way to access multimodal clinical data that...

    -   is PHI-approved

    -   displays information about provenance, lineage, and data governance (e.g., whether a column contains PHI, what access restrictions are on the data)

    -   supports best practices for dataset documentation

-   Cloud computing environments for managing statistical/machine learning workflows

-   Secure platform to publish and share deliverables (e.g. Quarto/Jupyter notebooks, dashboards, datasets)

-   A way to help users help themselves to expand capacity of the department

# Types of data used

-   Structured and unstructured data from the current EHR (Epic/Clarity) and historical EHR systems (ORCA/Cerner, etc.)

-   Cancer registry data (CNeXT)

-   Sunquest lab data

-   Mosaiq radiation oncology data

-   OnCore Clinical Trials Management (CTMS) system data

-   Pyxis medication administration

-   Gateway transplant and immunotherapy data

-   Novel, non-clinically reported data is relevant such as research use only genetic assay results

-   Survey and case report form type datasets

-   Validated lists of genomic data such as tumor mutations or structural variants

<div>

Image attribution: "[Women In Tech - 53](https://www.flickr.com/photos/136629440@N06/22344625928)" by [wocintechchat.com](https://www.flickr.com/photos/136629440@N06) is licensed under [CC BY 2.0](https://creativecommons.org/licenses/by/2.0/?ref=openverse).

</div>
